{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb3d78d",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [NYC Taxi Duration Prediction Pipeline](#toc1_)    \n",
    "  - [Project Overview](#toc1_1_)    \n",
    "  - [Environment Setup & Ingestion](#toc1_2_)    \n",
    "    - [Imports and Setup](#toc1_2_1_)    \n",
    "    - [Global Configuration (The SSoT Layer)](#toc1_2_2_)    \n",
    "    - [Experiment Tracking Setup (MLflow)](#toc1_2_3_)    \n",
    "    - [Data Ingestion Engine](#toc1_2_4_)    \n",
    "  - [Data Engineering & Preprocessing](#toc1_3_)    \n",
    "    - [Memory Management (SSoT for Data Types)](#toc1_3_1_)    \n",
    "  - [Data Transformation (Vectorization)](#toc1_4_)    \n",
    "  - [Baseline Modeling & Persistence](#toc1_5_)    \n",
    "    - [Model Persistence (Universal Serialization)](#toc1_5_1_)    \n",
    "  - [Model Validation (Out-of-Sample)](#toc1_6_)    \n",
    "  - [Experimentation with Advanced Models (XGBoost)](#toc1_7_)    \n",
    "    - [Exporting the Advanced Model (XGBoost)](#toc1_7_1_)    \n",
    "    - [Evaluation Analysis (November Validation Set)](#toc1_7_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf0275",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[NYC Taxi Duration Prediction Pipeline](#toc0_)\n",
    "**Author:** Ali Ahmed  \n",
    "**Role:** Associate ML/MLOps Engineer  \n",
    "**Contact:** [ðŸ“§ Email](mailto:ali.ahmed.nour14@gmail.com) | [ðŸ“± Phone](tel:+201007871314) | [ðŸ”— LinkedIn](https://www.linkedin.com/in/ali-ahmed-nour/)\n",
    "\n",
    "**Status:** Development / Production-Ready Simulation\n",
    "\n",
    "---\n",
    "\n",
    "## <a id='toc1_1_'></a>[Project Overview](#toc0_)\n",
    "This project implements a professional data pipeline designed with MLOps best practices:\n",
    "* **Automation Ready:** Modular code structure prepared for orchestration.\n",
    "* **Data Versioning Support:** Tiered storage (raw/processed) for better data lineage.\n",
    "* **Portability:** Environment-agnostic path management for seamless deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bac9f",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Environment Setup & Ingestion](#toc0_)\n",
    "*(Phase 1: Preparing tools, configurations, and fetching raw data)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67155ec5",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Imports and Setup](#toc0_)\n",
    "Library Initialization: Importation of necessary dependencies including XGBoost and CatBoost to ensure early resolution of environment requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15c9729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e/VScode/nyc-taxi-duration-mlops/.venv/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from typing import TypedDict, List, Dict, Any, cast  # Add List, Dict, Any, and cast\n",
    "\n",
    "# Third-party library imports\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Setting pandas display options for professional logging\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371ce4f",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Global Configuration (The SSoT Layer)](#toc0_)\n",
    "Global Configuration Layer: Establishment of a centralized Single Source of Truth (SSoT) to manage environment-specific variables, directory structures, and feature schemas.\n",
    "\n",
    "* **Environment Agnostic:** My code is designed to work seamlessly across Linux, macOS, and Windows.\n",
    "* **Scalable & Portable:** All paths (raw, processed, and model artifacts) as well as feature schemas are managed through a central configuration object. This allows for easy updates for different taxi types or time periods.\n",
    "* **Type-Safe:** I use `TypedDict` to provide explicit type hinting, which improves code maintainability and reduces runtime errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673079fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: SSoT Config initialized for yellow taxi (Period: 2025-10).\n",
      "LOG: Model will be saved as: model_lr_yellow_2025-10_v_2025-12-31.bin\n"
     ]
    }
   ],
   "source": [
    "# Updated Configuration to handle Dynamic Versioning\n",
    "class ProjectConfig(TypedDict):\n",
    "    taxi_type: str\n",
    "    year: int\n",
    "    month: int\n",
    "    data_url: str\n",
    "    raw_path: str\n",
    "    processed_path: str\n",
    "    model_path: str\n",
    "    categorical_features: list[str]\n",
    "    numerical_features: list[str]\n",
    "    all_features: list[str]\n",
    "    model_type: str  # Mandatory for multi-model SSoT\n",
    "    target: str\n",
    "\n",
    "\n",
    "def get_config(\n",
    "    taxi_type: str = \"yellow\", year: int = 2025, month: int = 10, model_type: str = \"lr\"\n",
    ") -> ProjectConfig:\n",
    "    \"\"\"\n",
    "    Generate a centralized configuration object for data and model paths.\n",
    "    Follows the Single Source of Truth (SSoT) principle.\n",
    "    \"\"\"\n",
    "    # 1. Base Directory Resolution (The Foundation)\n",
    "    base_dir = Path.cwd().parent\n",
    "    raw_dir = base_dir / \"data\" / \"raw\"\n",
    "    proc_dir = base_dir / \"data\" / \"processed\"\n",
    "    model_dir = base_dir / \"models\"\n",
    "\n",
    "    # 2. Versioning & Time Logic (The Context)\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # 3. Dynamic Name Construction (The Artifacts)\n",
    "    # Data filename (Input)\n",
    "    data_file = f\"{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet\"\n",
    "\n",
    "    # Model filename (Output) - Combines type, data period, and training date\n",
    "    model_name = f\"model_{model_type}_{taxi_type}_{year:04d}-{month:02d}_v_{today}.bin\"\n",
    "\n",
    "    # 4. Physical Directory Creation (The Execution)\n",
    "    for d in [raw_dir, proc_dir, model_dir]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 5. Feature Sets & Mapping (The Metadata)\n",
    "    cat_features = [\"PULocationID\", \"DOLocationID\"]\n",
    "    num_features = [\"trip_distance\"]\n",
    "\n",
    "    # 6. Final Object Assembly (The Result)\n",
    "    config: ProjectConfig = {\n",
    "        \"taxi_type\": taxi_type,\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"model_type\": model_type,\n",
    "        \"data_url\": f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{data_file}\",\n",
    "        \"raw_path\": str(raw_dir / data_file),\n",
    "        \"processed_path\": str(proc_dir / data_file),\n",
    "        \"model_path\": str(model_dir / model_name),\n",
    "        \"categorical_features\": cat_features,\n",
    "        \"numerical_features\": num_features,\n",
    "        \"all_features\": cat_features + num_features,\n",
    "        \"target\": \"duration\",\n",
    "    }\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "# 1. Defining the Universal Saver Function (Place this at the top with other functions)\n",
    "def save_artifact(cfg: ProjectConfig, model_obj, dv):\n",
    "    \"\"\"\n",
    "    Standardizes model saving across the notebook.\n",
    "    Ensures the (dv, model) bundle is always preserved.\n",
    "    \"\"\"\n",
    "    model_path = Path(cfg[\"model_path\"])\n",
    "\n",
    "    with model_path.open(\"wb\") as f_out:\n",
    "        pickle.dump((dv, model_obj), f_out)\n",
    "\n",
    "    print(f\"âœ… SUCCESS: {cfg['model_type'].upper()} model saved to: {model_path.name}\")\n",
    "    print(f\"ðŸ“¦ Size: {model_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "\n",
    "\n",
    "# Initialize Training Config\n",
    "cfg = get_config(year=2025, month=10)\n",
    "\n",
    "# Ruff fix: f-strings now contain variables, removing F541 warning\n",
    "print(\n",
    "    f\"LOG: SSoT Config initialized for {cfg['taxi_type']} taxi (Period: {cfg['year']}-{cfg['month']:02d}).\"\n",
    ")\n",
    "print(f\"LOG: Model will be saved as: {Path(cfg['model_path']).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedceb8",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[Experiment Tracking Setup (MLflow)](#toc0_)\n",
    "Configuration of the MLflow tracking infrastructure for metadata management. This setup ensures all model parameters, metrics, and artifacts are systematically logged to the local server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5253ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/31 09:39:26 INFO mlflow.tracking.fluent: Experiment with name 'nyc-taxi-duration-prediction' does not exist. Creating a new experiment.\n",
      "2025/12/31 09:39:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/12/31 09:39:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: MLflow Tracking URI: http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "# 1.3.1 Tracking Configuration\n",
    "# Setting the tracking URI to the local server\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "# 1.3.2 Experiment Initialization\n",
    "# Grouping all runs under the project name for systematic tracking\n",
    "mlflow.set_experiment(\"nyc-taxi-duration-prediction\")\n",
    "\n",
    "# 1.3.3 Pipeline Autologging\n",
    "# Enabling automatic logging for all supported libraries (XGBoost, Sklearn)\n",
    "mlflow.autolog()\n",
    "\n",
    "print(f\"LOG: MLflow Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915dbe1",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_4_'></a>[Data Ingestion Engine](#toc0_)\n",
    "In this stage, I implement an **Idempotent Ingestion Engine** to handle data loading. My goal is to fetch the NYC Taxi dataset from the official Cloudfront repository and store it in the `data/raw` directory. \n",
    "\n",
    "**Professional Standards Applied:**\n",
    "* **Idempotency & Caching:** The engine checks if the data already exists in the `raw_path` defined in our SSoT to avoid redundant downloads, saving bandwidth and execution time.\n",
    "* **Separation of Concerns:** The logic for \"where\" the data is and \"how\" to get it is encapsulated in a modular function, separated from the main execution flow.\n",
    "* **Automated Logging:** I included status updates to track the ingestion progress and file paths based on the centralized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e54827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Data already exists at yellow_tripdata_2025-10.parquet. Loading locally...\n",
      "LOG: Raw data shape: (4428699, 20)\n"
     ]
    }
   ],
   "source": [
    "def ingest_data(config: ProjectConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download raw parquet files if not present locally.\n",
    "    Implements idempotent data ingestion.\n",
    "    \"\"\"\n",
    "    # Use Path object for modern path manipulation\n",
    "    raw_path = Path(config[\"raw_path\"])\n",
    "\n",
    "    # Check if data exists in local storage to prevent redundant downloads\n",
    "    if not raw_path.exists():\n",
    "        # Attempt cloud data retrieval\n",
    "        try:\n",
    "            print(f\"LOG: Downloading data from {config['data_url']}...\")\n",
    "            df = pd.read_parquet(config[\"data_url\"])\n",
    "\n",
    "            # Ensure the directory structure is ready before saving\n",
    "            raw_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            df.to_parquet(raw_path)\n",
    "            print(f\"LOG: Data successfully cached at: {raw_path.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to fetch data from cloud. Exception: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        # Load directly from cache if available\n",
    "        print(f\"LOG: Data already exists at {raw_path.name}. Loading locally...\")\n",
    "        df = pd.read_parquet(raw_path)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Executing ingestion using the October 2025 SSoT config\n",
    "df = ingest_data(cfg)\n",
    "print(f\"LOG: Raw data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986498f",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Data Engineering & Preprocessing](#toc0_)\n",
    "*(Phase 2: Cleaning, transforming, and optimizing the dataset)*\n",
    "\n",
    "Data Engineering Phase: Transformation of raw taxi records into machine-learning-ready features through duration derivation and outlier mitigation.\n",
    "\n",
    "* **SSoT Feature Mapping:** Instead of hard-coding column names, I dynamically pull them from the `ProjectConfig`. This ensures that any schema changes in the source data only need to be updated in one place.\n",
    "* **Target Engineering:** I derive the `duration` variable from pickup and dropoff timestamps and mitigate outliers by filtering for trips between 1 and 60 minutes.\n",
    "* **Memory Optimization & Type Integrity:** I convert categorical location IDs to strings. This ensures the `DictVectorizer` treats them as discrete entities rather than continuous numbers, while maintaining a memory-efficient workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3843ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Starting Data Engineering and Preprocessing...\n",
      "LOG: Preprocessing complete.\n",
      "LOG: Final Shape: (4198802, 21)\n",
      "LOG: Average Duration: 17.32 minutes\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(df: pd.DataFrame, config: ProjectConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform data cleaning, feature engineering, and outlier filtering.\n",
    "    Enforcement of Single Source of Truth (SSoT) feature types is applied.\n",
    "    \"\"\"\n",
    "    # 1. Target Derivation: Conversion of pickup/dropoff timestamps to duration in minutes\n",
    "    df[\"duration\"] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df[\"duration\"] = df[\"duration\"].apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    # 2. Outlier Mitigation: Filtration to maintain operational range (1-60 minutes)\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    # 3. Categorical Integrity: Casting feature identifiers to string for consistent vectorization\n",
    "    # Feature names are dynamically retrieved from the centralized ProjectConfig\n",
    "    cat_features = config[\"categorical_features\"]\n",
    "    df[cat_features] = df[cat_features].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Execution of Data Engineering Phase\n",
    "print(\"LOG: Starting Data Engineering and Preprocessing...\")\n",
    "df_processed = preprocess_data(df, cfg)\n",
    "\n",
    "print(\"LOG: Preprocessing complete.\")\n",
    "print(f\"LOG: Final Shape: {df_processed.shape}\")\n",
    "print(f\"LOG: Average Duration: {df_processed.duration.mean():.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a36f2",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[Memory Management (SSoT for Data Types)](#toc0_)\n",
    "To handle millions of rows efficiently, I enforce a strict schema for data types. This optimization phase ensures the pipeline remains scalable:\n",
    "\n",
    "* **Vectorized Optimization:** I use a vectorized approach to downcast numerical features, ensuring high performance and significantly reducing the CPU overhead.\n",
    "* **Consistency & Footprint:** This ensures that both training and validation data occupy the minimum memory footprint (e.g., using `float32` instead of `float64`), which is critical for cloud-based training environments.\n",
    "* **SSoT Integration:** The function dynamically identifies numerical columns and the target from the centralized `ProjectConfig`, maintaining a Single Source of Truth for the entire data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82634f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Memory Optimization Report for yellow dataset:\n",
      "   - BEFORE: 1183.29 MB\n",
      "   - AFTER: 1151.25 MB\n",
      "   - Reduction: 2.71%\n"
     ]
    }
   ],
   "source": [
    "def optimize_memory_vectorized(df: pd.DataFrame, config: ProjectConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Optimizes memory usage by downcasting numerical types based on SSoT config.\n",
    "    \"\"\"\n",
    "    # 1. Calculate memory BEFORE optimization\n",
    "    mem_before = df.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    # 2. Identify numerical columns from SSoT (Features + Target)\n",
    "    # This prevents hard-coding and respects the schema defined in Section 1.2\n",
    "    num_cols = config[\"numerical_features\"] + [config[\"target\"]]\n",
    "\n",
    "    # 3. Downcast to float32 (Industry standard for ML precision/memory balance)\n",
    "    df[num_cols] = df[num_cols].astype(\"float32\")\n",
    "\n",
    "    # 4. Calculate memory AFTER optimization\n",
    "    mem_after = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    improvement = ((mem_before - mem_after) / mem_before) * 100\n",
    "\n",
    "    print(f\"LOG: Memory Optimization Report for {config['taxi_type']} dataset:\")\n",
    "    print(f\"   - BEFORE: {mem_before:.2f} MB\")\n",
    "    print(f\"   - AFTER: {mem_after:.2f} MB\")\n",
    "    print(f\"   - Reduction: {improvement:.2f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Execute Memory Optimization\n",
    "df_processed = optimize_memory_vectorized(df_processed, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c9d0d",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Data Transformation (Vectorization)](#toc0_)\n",
    "*(Phase 3: Converting dataframes into numerical matrices)*\n",
    "\n",
    "In this stage, I convert the processed categorical and numerical features into a format that the machine learning model can understand. This transformation is crucial for several reasons:\n",
    "\n",
    "* **One-Hot Encoding & Efficiency:** I use `DictVectorizer` to handle categorical location IDs, which creates a **sparse matrix**. This optimizes memory usage and ensures the model can interpret discrete categories correctly.\n",
    "* **SSoT Alignment:** Instead of manually selecting columns, I dynamically utilize the `all_features` list from the `ProjectConfig`. This guarantees a consistent and reproducible feature order.\n",
    "* **Consistency for Production:** The `DictVectorizer` (dv) fitted here becomes the **\"Source of Truth\"** for all future data (Validation/Production), ensuring that feature schemas remain synchronized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee9622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Feature Transformation complete.\n",
      "LOG: Training Matrix Shape: (4198802, 523)\n",
      "LOG: DictVectorizer successfully mapped 523 features.\n"
     ]
    }
   ],
   "source": [
    "# 1. Feature & Target Selection using SSoT\n",
    "# We pull definitions directly from the config object\n",
    "features = cfg[\"all_features\"]\n",
    "target = cfg[\"target\"]\n",
    "\n",
    "# 2. Convert DataFrame to List of Dictionaries\n",
    "# This is a memory-efficient way to pass data to the DictVectorizer\n",
    "train_dicts = df_processed[features].to_dict(orient=\"records\")\n",
    "\n",
    "# 3. Fit and Transform the Vectorizer\n",
    "# 'dv' will be saved later to be used in the inference pipeline\n",
    "dv = DictVectorizer()\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "# 4. Extract Target Vector\n",
    "y_train = df_processed[target].to_numpy()\n",
    "\n",
    "print(\"LOG: Feature Transformation complete.\")\n",
    "print(f\"LOG: Training Matrix Shape: {X_train.shape}\")\n",
    "print(f\"LOG: DictVectorizer successfully mapped {len(dv.feature_names_)} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577f929",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Baseline Modeling & Persistence](#toc0_)\n",
    "*(Phase 4: Establishing a performance benchmark and managing artifacts)*\n",
    "\n",
    "In this final stage, I establish a **Baseline** for the project and ensure the persistence of the training results following our **Single Source of Truth (SSoT)** architecture:\n",
    "\n",
    "* **Baseline Establishment & RMSE:** I train a **Linear Regression** model to serve as our performance benchmark. Using Root Mean Squared Error (RMSE), I establish a \"score to beat\" to verify the predictive power of features dynamically pulled from the `ProjectConfig`.\n",
    "* **Artifact Synchronization:** I serialize both the model and the `DictVectorizer` into a single binary file. This is a critical MLOps practice that prevents **\"Schema Skew\"** during inference by ensuring the preprocessor and model remain perfectly synchronized.\n",
    "* **SSoT Persistence:** I utilize the `model_path` defined in our centralized configuration to ensure artifacts are stored automatically in the correct project directory (`models/`), ensuring reproducibility and pipeline alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5402502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Baseline model (LinearRegression) training complete.\n",
      "LOG: Training RMSE: 9.55 minutes\n"
     ]
    }
   ],
   "source": [
    "# 1. Model Configuration\n",
    "# Ensuring the Baseline model uses 'lr' type for proper SSoT pathing\n",
    "cfg = get_config(taxi_type=\"yellow\", year=2025, month=10, model_type=\"lr\")\n",
    "\n",
    "# 2. Model Initialization\n",
    "# Establishing a simple Linear Regression as the performance benchmark\n",
    "lr = LinearRegression()\n",
    "\n",
    "# 3. Model Training\n",
    "# SSoT: Training using the October feature matrix (X_train)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# 4. Training Evaluation\n",
    "# Checking how well the model fits the training data\n",
    "y_pred_train = lr.predict(X_train)\n",
    "rmse_lr_train = root_mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "print(f\"LOG: Baseline model ({lr.__class__.__name__}) training complete.\")\n",
    "print(f\"LOG: Training RMSE: {rmse_lr_train:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ead19",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_1_'></a>[Model Persistence (Universal Serialization)](#toc0_)\n",
    "\n",
    "In a production-grade pipeline, saving the model is not just about writing a file; it's about ensuring the **reproducibility** of the entire inference logic.\n",
    "\n",
    "* **Unified Saver:** We use a centralized `save_artifact` function to handle the serialization of any model type (Baseline or XGBoost) through a unified interface.\n",
    "* **The \"Inference Bundle\":** To prevent **Training-Serving Skew**, we bundle the `DictVectorizer` (the feature schema) with the model object. This ensures that the 523+ features generated during training are mapped identically during future predictions.\n",
    "* **Dynamic Versioning:** The file name is automatically derived from the `ProjectConfig` (SSoT), incorporating the model type, data period, and training date for full traceability.\n",
    "* **Pathlib Integration:** Using `pathlib` ensures cross-platform compatibility and clean directory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8553cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SUCCESS: LR model saved to: model_lr_yellow_2025-10_v_2025-12-30.bin\n",
      "ðŸ“¦ Size: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "# 6. Artifact Persistence\n",
    "# Save the model and dict vectorizer using the centralized function\n",
    "save_artifact(cfg, lr, dv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089c35c",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Model Validation (Out-of-Sample)](#toc0_)\n",
    "In this phase, I evaluate the model's performance on unseen data (November 2025). \n",
    "* **The Goal:** To ensure the model generalizes well to new data and maintains a similar RMSE to the training phase.\n",
    "* **The Pipeline:** I will reuse the `prepare_features` and `optimize_memory_vectorized` functions to maintain consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f54c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Starting validation pipeline for 11/2025...\n",
      "LOG: Data already exists at yellow_tripdata_2025-11.parquet. Loading locally...\n",
      "LOG: Memory Optimization Report for yellow dataset:\n",
      "   - BEFORE: 1118.86 MB\n",
      "   - AFTER: 1088.52 MB\n",
      "   - Reduction: 2.71%\n",
      "LOG: Validation complete for yellow taxi.\n",
      "LOG: Linear Regression Validation RMSE: 9.48 minutes\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Validation Configuration for November 2025\n",
    "# Centralized configuration ensures we point to the correct validation file\n",
    "val_cfg = get_config(year=2025, month=11)\n",
    "\n",
    "# 2. Ingest and Prepare Validation Data\n",
    "print(f\"LOG: Starting validation pipeline for {val_cfg['month']}/{val_cfg['year']}...\")\n",
    "df_val_raw = ingest_data(val_cfg)\n",
    "\n",
    "# 3. Preprocessing and Memory Optimization\n",
    "df_val = preprocess_data(df_val_raw, val_cfg)\n",
    "df_val = optimize_memory_vectorized(df_val, val_cfg)\n",
    "\n",
    "# 4. Feature Transformation (Transform only using fitted DV)\n",
    "# We cast types to ensure stability and use iterator for memory efficiency\n",
    "val_dicts = df_val[val_cfg[\"all_features\"]].to_dict(orient=\"records\")\n",
    "X_val = dv.transform(iter(cast(List[Dict[str, Any]], val_dicts)))\n",
    "y_val = df_val[val_cfg[\"target\"]].to_numpy()\n",
    "\n",
    "# 5. Baseline (Linear Regression) Validation Evaluation\n",
    "# Using the model 'lr' trained in the previous cell\n",
    "y_pred_lr_val = lr.predict(X_val)\n",
    "rmse_lr_val = root_mean_squared_error(y_val, y_pred_lr_val)\n",
    "\n",
    "print(f\"LOG: Validation complete for {val_cfg['taxi_type']} taxi.\")\n",
    "print(f\"LOG: Linear Regression Validation RMSE: {rmse_lr_val:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7708479",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Experimentation with Advanced Models (XGBoost)](#toc0_)\n",
    "This phase focuses on advancing beyond Linear Regression to capture non-linear patterns within the NYC taxi dataset. The primary goal is to evaluate if a gradient boosting approach can improve the baseline RMSE (9.48).\n",
    "\n",
    "* **Model:** XGBoost (Extreme Gradient Boosting).\n",
    "* **Evaluation:** Comparison of validation RMSE against the linear baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904e0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-rmse:9.10641\n",
      "[10]\tvalidation-rmse:6.65067\n",
      "[20]\tvalidation-rmse:6.56195\n",
      "[30]\tvalidation-rmse:6.51450\n",
      "[40]\tvalidation-rmse:6.47538\n",
      "[50]\tvalidation-rmse:6.44804\n",
      "[60]\tvalidation-rmse:6.42301\n",
      "[70]\tvalidation-rmse:6.40096\n",
      "[80]\tvalidation-rmse:6.38448\n",
      "[90]\tvalidation-rmse:6.36894\n",
      "[99]\tvalidation-rmse:6.35575\n",
      "LOG: Linear Regression Validation RMSE: 9.48\n",
      "LOG: XGBoost Validation RMSE: 6.36 minutes\n"
     ]
    }
   ],
   "source": [
    "# 1. Specialized data structure for XGBoost (No new transformations)\n",
    "# These DMatrix objects reference the existing X_train and X_val matrices\n",
    "train_xgb = xgb.DMatrix(X_train, label=y_train)\n",
    "val_xgb = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# 2. Define hyperparameters\n",
    "params = {\"max_depth\": 6, \"objective\": \"reg:squarederror\", \"nthread\": 8, \"seed\": 42}\n",
    "\n",
    "# 3. Model Training Logic\n",
    "# Booster training using the established validation set\n",
    "booster = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=train_xgb,\n",
    "    num_boost_round=100,\n",
    "    evals=[(val_xgb, \"validation\")],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=10,\n",
    ")\n",
    "\n",
    "# 4. Evaluation (Consistency check)\n",
    "# Updated variable names to match the dynamic summary table\n",
    "y_pred_xgb_val = booster.predict(val_xgb)\n",
    "rmse_xgb_val = root_mean_squared_error(y_val, y_pred_xgb_val)\n",
    "\n",
    "# Also calculating training RMSE for the full comparison table\n",
    "y_pred_xgb_train = booster.predict(train_xgb)\n",
    "rmse_xgb_train = root_mean_squared_error(y_train, y_pred_xgb_train)\n",
    "\n",
    "print(f\"LOG: Linear Regression Validation RMSE: {rmse_lr_val:.2f}\")\n",
    "print(f\"LOG: XGBoost Validation RMSE: {rmse_xgb_val:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de4c51",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_1_'></a>[Exporting the Advanced Model (XGBoost)](#toc0_)\n",
    "\n",
    "Following the tuning and training of the XGBoost booster, the model is persisted using the universal interface. \n",
    "Updating the `model_type` within the configuration triggers the system to automatically route the artifact to the designated path, adhering to the established naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea6200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SUCCESS: XGB model saved to: model_xgb_yellow_2025-10_v_2025-12-30.bin\n",
      "ðŸ“¦ Size: 0.42 MB\n"
     ]
    }
   ],
   "source": [
    "cfg = get_config(model_type=\"xgb\", year=2025, month=10)\n",
    "# ... Ø¨Ø¹Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ...\n",
    "save_artifact(cfg, booster, dv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa2097",
   "metadata": {},
   "source": [
    "### <a id='toc1_7_2_'></a>[Evaluation Analysis (November Validation Set)](#toc0_)\n",
    "The model performance was evaluated using the November 2025 dataset, ensuring an out-of-sample validation consistent with the previous linear baseline. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5969eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Model | Training RMSE | Validation RMSE | Improvement (Val) |\n",
       "| :--- | :--- | :--- | :--- |\n",
       "| **Linear Regression** | 9.55 | 9.48 | Reference |\n",
       "| **XGBoost** | 6.36 | 6.36 | ~32.9% |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Calculate improvement percentage\n",
    "improvement = (rmse_lr_val - rmse_xgb_val) / rmse_lr_val * 100\n",
    "\n",
    "table_content = f\"\"\"\n",
    "| Model | Training RMSE | Validation RMSE | Improvement (Val) |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Linear Regression** | {rmse_lr_train:.2f} | {rmse_lr_val:.2f} | Reference |\n",
    "| **XGBoost** | {rmse_xgb_train:.2f} | {rmse_xgb_val:.2f} | ~{improvement:.1f}% |\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(table_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d84110",
   "metadata": {},
   "source": [
    "\n",
    "**Key Finding:** XGBoost successfully captured high-dimensional interactions between location IDs that were ignored by the linear model, leading to a significant reduction in prediction error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-taxi-duration-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
