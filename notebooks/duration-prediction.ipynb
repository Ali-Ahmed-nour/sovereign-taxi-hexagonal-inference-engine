{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4bf0275",
   "metadata": {},
   "source": [
    "# NYC Taxi Duration Prediction Pipeline\n",
    "**Author:** Ali Ahmed  \n",
    "**Role:** Associate ML/MLOps Engineer  \n",
    "**Contact:** [ðŸ“§ Email](mailto:ali.ahmed.nour14@gmail.com) | [ðŸ“± Phone](tel:+201007871314) | [ðŸ”— LinkedIn](https://www.linkedin.com/in/ali-ahmed-nour/)\n",
    "\n",
    "**Status:** Development / Production-Ready Simulation\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "This project implements a professional data pipeline designed with MLOps best practices:\n",
    "* **Automation Ready:** Modular code structure prepared for orchestration.\n",
    "* **Data Versioning Support:** Tiered storage (raw/processed) for better data lineage.\n",
    "* **Portability:** Environment-agnostic path management for seamless deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fb563",
   "metadata": {},
   "source": [
    "## ðŸš• Table of Contents\n",
    "1. [Environment Setup & Ingestion](#1.-Environment-Setup-&-Ingestion)\n",
    "    * [1.1 Imports and Setup](#1.1-Imports-and-Setup)\n",
    "    * [1.2 Global Configuration (The SSoT Layer)](#1.2-Global-Configuration)\n",
    "    * [1.3 Data Ingestion Engine](#1.3-Data-Ingestion-Engine)\n",
    "2. [Data Engineering & Preprocessing](#2.-Data-Engineering-&-Preprocessing)\n",
    "    * [2.1 Feature Engineering Logic](#2.1-Feature-Engineering-Logic)\n",
    "    * [2.2 Memory Optimization (SSoT for Dtypes)](#2.2-Memory-Optimization)\n",
    "3. [Feature Transformation & Vectorization](#3.-Feature-Transformation)\n",
    "4. [Baseline Modeling: Linear Regression](#4.-Baseline-Modeling:-Linear-Regression)\n",
    "    * [4.1 Model Training & Baseline RMSE](#4.1-Model-Training-&-Baseline-RMSE)\n",
    "    * [4.2 Model Persistence (Artifact Management)](#4.2-Model-Persistence)\n",
    "5. [Model Validation (Out-of-Sample)](#5.-Model-Validation-(Out-of-Sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bac9f",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Ingestion\n",
    "*(Phase 1: Preparing tools, configurations, and fetching raw data)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67155ec5",
   "metadata": {},
   "source": [
    "### 1.1 Imports and Setup\n",
    "In this section, I import all necessary libraries. I include **XGBoost** and **CatBoost** from the start to ensure all dependencies are resolved early for the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e15c9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from typing import TypedDict, List, Dict, Any, cast  # Add List, Dict, Any, and cast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import xgboost as xgb\n",
    "# from catboost import CatBoostRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Setting pandas display options for professional logging\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371ce4f",
   "metadata": {},
   "source": [
    "### 1.2 Global Configuration (The SSoT Layer)\n",
    "To avoid hard-coding and ensure consistency, I've established a **Single Source of Truth (SSoT)**. In this section, I establish a centralized configuration layer to manage environment-specific variables. By defining a `BASE_DIR` and utilizing `os.path.join`, I ensure the pipeline remains:\n",
    "\n",
    "* **Environment Agnostic:** My code is designed to work seamlessly across Linux, macOS, and Windows.\n",
    "* **Scalable & Portable:** All paths (raw, processed, and model artifacts) as well as feature schemas are managed through a central configuration object. This allows for easy updates for different taxi types or time periods.\n",
    "* **Type-Safe:** I use `TypedDict` to provide explicit type hinting, which improves code maintainability and reduces runtime errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "673079fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: SSoT Config initialized for yellow taxi (Period: 2025-10).\n",
      "LOG: Model will be saved as: model_yellow_2025-10_v_2025-12-29.bin\n"
     ]
    }
   ],
   "source": [
    "# Updated Configuration to handle Dynamic Versioning\n",
    "class ProjectConfig(TypedDict):\n",
    "    taxi_type: str\n",
    "    year: int\n",
    "    month: int\n",
    "    data_url: str\n",
    "    raw_path: str\n",
    "    processed_path: str\n",
    "    model_path: str\n",
    "    categorical_features: list[str]\n",
    "    numerical_features: list[str]\n",
    "    all_features: list[str]\n",
    "    target: str\n",
    "\n",
    "\n",
    "def get_config(\n",
    "    taxi_type: str = \"yellow\", year: int = 2025, month: int = 10\n",
    ") -> ProjectConfig:\n",
    "    \"\"\"\n",
    "    Centralized configuration with Dynamic Date Versioning for artifacts.\n",
    "    \"\"\"\n",
    "    # Root directory resolution for WSL structure\n",
    "    base_dir = Path.cwd().parent\n",
    "\n",
    "    # 1. Versioning Logic: Get current date (SSoT for file naming)\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Path definitions matching your terminal tree\n",
    "    raw_dir = base_dir / \"data\" / \"raw\"\n",
    "    proc_dir = base_dir / \"data\" / \"processed\"\n",
    "    model_dir = base_dir / \"models\"\n",
    "\n",
    "    # Ensure structure exists using Pathlib\n",
    "    for d in [raw_dir, proc_dir, model_dir]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    file_name = f\"{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet\"\n",
    "\n",
    "    # 2. Dynamic Model Name: Includes training month AND current date\n",
    "    model_name = f\"model_{taxi_type}_{year:04d}-{month:02d}_v_{today}.bin\"\n",
    "\n",
    "    # SSoT Feature sets\n",
    "    cat_features = [\"PULocationID\", \"DOLocationID\"]\n",
    "    num_features = [\"trip_distance\"]\n",
    "\n",
    "    # We define the config object explicitly to ensure strict type compliance\n",
    "    config: ProjectConfig = {\n",
    "        \"taxi_type\": taxi_type,\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"data_url\": f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file_name}\",\n",
    "        \"raw_path\": str(raw_dir / file_name),\n",
    "        \"processed_path\": str(proc_dir / file_name),\n",
    "        \"model_path\": str(model_dir / model_name),\n",
    "        \"categorical_features\": cat_features,\n",
    "        \"numerical_features\": num_features,\n",
    "        \"all_features\": cat_features + num_features,\n",
    "        \"target\": \"duration\",\n",
    "    }\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "# Initialize Training Config\n",
    "cfg = get_config(year=2025, month=10)\n",
    "\n",
    "# Ruff fix: f-strings now contain variables, removing F541 warning\n",
    "print(\n",
    "    f\"LOG: SSoT Config initialized for {cfg['taxi_type']} taxi (Period: {cfg['year']}-{cfg['month']:02d}).\"\n",
    ")\n",
    "print(f\"LOG: Model will be saved as: {Path(cfg['model_path']).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915dbe1",
   "metadata": {},
   "source": [
    "### 1.3 Data Ingestion Engine\n",
    "In this stage, I implement an **Idempotent Ingestion Engine** to handle data loading. My goal is to fetch the NYC Taxi dataset from the official Cloudfront repository and store it in the `data/raw` directory. \n",
    "\n",
    "**Professional Standards Applied:**\n",
    "* **Idempotency & Caching:** The engine checks if the data already exists in the `raw_path` defined in our SSoT to avoid redundant downloads, saving bandwidth and execution time.\n",
    "* **Separation of Concerns:** The logic for \"where\" the data is and \"how\" to get it is encapsulated in a modular function, separated from the main execution flow.\n",
    "* **Automated Logging:** I included status updates to track the ingestion progress and file paths based on the centralized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9e54827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Data already exists at yellow_tripdata_2025-10.parquet. Loading locally...\n",
      "LOG: Raw data shape: (4428699, 20)\n"
     ]
    }
   ],
   "source": [
    "def ingest_data(config: ProjectConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads and loads the parquet data based on SSoT config using Pathlib.\n",
    "    Includes caching and error handling for production environments.\n",
    "    \"\"\"\n",
    "    # Use Path object for modern path manipulation\n",
    "    raw_path = Path(config[\"raw_path\"])\n",
    "\n",
    "    # Check if data exists in local storage\n",
    "    if not raw_path.exists():\n",
    "        try:\n",
    "            print(f\"LOG: Downloading data from {config['data_url']}...\")\n",
    "            df = pd.read_parquet(config[\"data_url\"])\n",
    "\n",
    "            # Ensure the directory structure is ready before saving\n",
    "            raw_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            df.to_parquet(raw_path)\n",
    "            print(f\"LOG: Data successfully cached at: {raw_path.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to fetch data from cloud. Exception: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        # Load directly from cache if available\n",
    "        print(f\"LOG: Data already exists at {raw_path.name}. Loading locally...\")\n",
    "        df = pd.read_parquet(raw_path)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Executing ingestion using the October 2025 SSoT config\n",
    "df = ingest_data(cfg)\n",
    "print(f\"LOG: Raw data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986498f",
   "metadata": {},
   "source": [
    "## 2. Data Engineering & Preprocessing\n",
    "*(Phase 2: Cleaning, transforming, and optimizing the dataset)*\n",
    "\n",
    "In this stage, I transform raw taxi records into a format suitable for machine learning using high professional standards:\n",
    "* **SSoT Feature Mapping:** Instead of hard-coding column names, I dynamically pull them from the `ProjectConfig`. This ensures that any schema changes in the source data only need to be updated in one place.\n",
    "* **Target Engineering:** I derive the `duration` variable from pickup and dropoff timestamps and mitigate outliers by filtering for trips between 1 and 60 minutes.\n",
    "* **Memory Optimization & Type Integrity:** I convert categorical location IDs to strings. This ensures the `DictVectorizer` treats them as discrete entities rather than continuous numbers, while maintaining a memory-efficient workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3843ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Starting Data Engineering and Preprocessing...\n",
      "LOG: Preprocessing complete.\n",
      "LOG: Final Shape: (4198802, 21)\n",
      "LOG: Average Duration: 17.32 minutes\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(df: pd.DataFrame, config: ProjectConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans data, derives target, and enforces SSoT feature types.\n",
    "    \"\"\"\n",
    "    # 1. Target Derivation: Calculate duration in minutes\n",
    "    df[\"duration\"] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df[\"duration\"] = df[\"duration\"].apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    # 2. Outlier Mitigation (Operational Range: 1-60 mins)\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    # 3. Memory Optimization & Categorical Integrity\n",
    "    # Dynamically fetching feature names from SSoT config\n",
    "    cat_features = config[\"categorical_features\"]\n",
    "    df[cat_features] = df[cat_features].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Start Data Engineering Phase\n",
    "print(\"LOG: Starting Data Engineering and Preprocessing...\")\n",
    "df_processed = preprocess_data(df, cfg)\n",
    "\n",
    "print(\"LOG: Preprocessing complete.\")\n",
    "print(f\"LOG: Final Shape: {df_processed.shape}\")\n",
    "print(f\"LOG: Average Duration: {df_processed.duration.mean():.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a36f2",
   "metadata": {},
   "source": [
    "### 2.2 Memory Management (SSoT for Data Types)\n",
    "To handle millions of rows efficiently, I enforce a strict schema for data types. This optimization phase ensures the pipeline remains scalable:\n",
    "\n",
    "* **Vectorized Optimization:** I use a vectorized approach to downcast numerical features, ensuring high performance and significantly reducing the CPU overhead.\n",
    "* **Consistency & Footprint:** This ensures that both training and validation data occupy the minimum memory footprint (e.g., using `float32` instead of `float64`), which is critical for cloud-based training environments.\n",
    "* **SSoT Integration:** The function dynamically identifies numerical columns and the target from the centralized `ProjectConfig`, maintaining a Single Source of Truth for the entire data schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e82634f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Memory Optimization Report:\n",
      "   - BEFORE: 1183.29 MB\n",
      "   - AFTER: 1151.25 MB\n",
      "   - Improvement Percentage: 2.71%\n"
     ]
    }
   ],
   "source": [
    "def optimize_memory_vectorized(df: pd.DataFrame, config: ProjectConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Optimizes memory usage by downcasting numerical types based on SSoT config.\n",
    "    \"\"\"\n",
    "    # 1. Calculate memory BEFORE optimization\n",
    "    mem_before = df.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    # 2. Identify numerical columns from SSoT (Features + Target)\n",
    "    # This prevents hard-coding and respects the schema defined in Section 1.2\n",
    "    num_cols = config[\"numerical_features\"] + [config[\"target\"]]\n",
    "\n",
    "    # 3. Downcast to float32 (Industry standard for ML precision/memory balance)\n",
    "    df[num_cols] = df[num_cols].astype(\"float32\")\n",
    "\n",
    "    # 4. Calculate memory AFTER optimization\n",
    "    mem_after = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    improvement = ((mem_before - mem_after) / mem_before) * 100\n",
    "\n",
    "    print(f\"LOG: Memory Optimization Report for {config['taxi_type']} dataset:\")\n",
    "    print(f\"   - BEFORE: {mem_before:.2f} MB\")\n",
    "    print(f\"   - AFTER: {mem_after:.2f} MB\")\n",
    "    print(f\"   - Reduction: {improvement:.2f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Execute Memory Optimization\n",
    "df_processed = optimize_memory_vectorized(df_processed, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c9d0d",
   "metadata": {},
   "source": [
    "## 3. Data Transformation (Vectorization)\n",
    "*(Phase 3: Converting dataframes into numerical matrices)*\n",
    "\n",
    "In this stage, I convert the processed categorical and numerical features into a format that the machine learning model can understand. This transformation is crucial for several reasons:\n",
    "\n",
    "* **One-Hot Encoding & Efficiency:** I use `DictVectorizer` to handle categorical location IDs, which creates a **sparse matrix**. This optimizes memory usage and ensures the model can interpret discrete categories correctly.\n",
    "* **SSoT Alignment:** Instead of manually selecting columns, I dynamically utilize the `all_features` list from the `ProjectConfig`. This guarantees a consistent and reproducible feature order.\n",
    "* **Consistency for Production:** The `DictVectorizer` (dv) fitted here becomes the **\"Source of Truth\"** for all future data (Validation/Production), ensuring that feature schemas remain synchronized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee9622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Feature Transformation complete.\n",
      "LOG: Training Matrix Shape: (4198802, 523)\n",
      "LOG: DictVectorizer successfully mapped 523 features.\n"
     ]
    }
   ],
   "source": [
    "# 1. Feature & Target Selection using SSoT\n",
    "# We pull definitions directly from the config object\n",
    "features = cfg[\"all_features\"]\n",
    "target = cfg[\"target\"]\n",
    "\n",
    "# 2. Convert DataFrame to List of Dictionaries\n",
    "# This is a memory-efficient way to pass data to the DictVectorizer\n",
    "train_dicts = df_processed[features].to_dict(orient=\"records\")\n",
    "\n",
    "# 3. Fit and Transform the Vectorizer\n",
    "# 'dv' will be saved later to be used in the inference pipeline\n",
    "dv = DictVectorizer()\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "# 4. Extract Target Vector\n",
    "y_train = df_processed[target].to_numpy()\n",
    "\n",
    "print(\"LOG: Feature Transformation complete.\")\n",
    "print(f\"LOG: Training Matrix Shape: {X_train.shape}\")\n",
    "print(f\"LOG: DictVectorizer successfully mapped {len(dv.feature_names_)} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577f929",
   "metadata": {},
   "source": [
    "## 4. Baseline Modeling & Persistence\n",
    "*(Phase 4: Establishing a performance benchmark and managing artifacts)*\n",
    "\n",
    "In this final stage, I establish a **Baseline** for the project and ensure the persistence of the training results following our **Single Source of Truth (SSoT)** architecture:\n",
    "\n",
    "* **Baseline Establishment & RMSE:** I train a **Linear Regression** model to serve as our performance benchmark. Using Root Mean Squared Error (RMSE), I establish a \"score to beat\" to verify the predictive power of features dynamically pulled from the `ProjectConfig`.\n",
    "* **Artifact Synchronization:** I serialize both the model and the `DictVectorizer` into a single binary file. This is a critical MLOps practice that prevents **\"Schema Skew\"** during inference by ensuring the preprocessor and model remain perfectly synchronized.\n",
    "* **SSoT Persistence:** I utilize the `model_path` defined in our centralized configuration to ensure artifacts are stored automatically in the correct project directory (`models/`), ensuring reproducibility and pipeline alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5402502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Baseline model (LinearRegression) training complete.\n",
      "LOG: Training RMSE for yellow taxi: 9.55 minutes\n"
     ]
    }
   ],
   "source": [
    "# 1. Model Initialization\n",
    "# Linear Regression provides a fast, interpretable benchmark\n",
    "lr = LinearRegression()\n",
    "\n",
    "# 2. Model Training\n",
    "# SSoT: Training on the feature matrix generated in Section 3\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# 3. Evaluation\n",
    "# Calculating the 'score to beat' for future model iterations\n",
    "y_pred = lr.predict(X_train)\n",
    "rmse_train = root_mean_squared_error(y_train, y_pred)\n",
    "\n",
    "print(f\"LOG: Baseline model ({lr.__class__.__name__}) training complete.\")\n",
    "print(f\"LOG: Training RMSE for {cfg['taxi_type']} taxi: {rmse_train:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639ead19",
   "metadata": {},
   "source": [
    "### 4.2 Model Persistence (Serialization)\n",
    "In a production environment, it is crucial to save the trained model and the vectorizer to maintain consistency.\n",
    "\n",
    "* **The \"Source of Truth\" Bundle:** I save both the `DictVectorizer` (which holds the feature schema) and the `LinearRegression` model as a single binary file using `pickle`.\n",
    "* **Standardization:** This ensures that during inference or validation, we use the exact same feature mapping (the 523 indices in our case) that the model was trained on, preventing **Training-Serving Skew**.\n",
    "* **Reproducibility:** Saving these artifacts allows us to load the model in other environments without needing to re-run the entire training pipeline, ensuring the portability of our SSoT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8553cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Artifact already exists. Overwriting current version: model_yellow_2025-10_v_2025-12-29.bin\n",
      "LOG: Successfully saved to: /mnt/e/VScode/nyc-taxi-duration-mlops/models/model_yellow_2025-10_v_2025-12-29.bin\n",
      "LOG: Final Artifact size: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "# 1. Path Resolution from SSoT (Casting to Path object)\n",
    "model_path = Path(cfg[\"model_path\"])\n",
    "\n",
    "# 2. Logic: Overwrite with Logging using Pathlib properties\n",
    "if model_path.exists():\n",
    "    print(\n",
    "        f\"INFO: Artifact already exists. Overwriting current version: {model_path.name}\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"LOG: Creating new artifact: {model_path.name}\")\n",
    "\n",
    "# 3. Serialization (Using pathlib's context manager for cleaner I/O)\n",
    "with model_path.open(\"wb\") as f_out:\n",
    "    pickle.dump((dv, lr), f_out)\n",
    "\n",
    "# 4. Verification using Pathlib stat() method\n",
    "artifact_size = model_path.stat().st_size / 1024**2\n",
    "print(f\"LOG: Successfully saved to: {model_path}\")\n",
    "print(f\"LOG: Final Artifact size: {artifact_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089c35c",
   "metadata": {},
   "source": [
    "## 5. Model Validation (Out-of-Sample)\n",
    "In this phase, I evaluate the model's performance on unseen data (November 2025). \n",
    "* **The Goal:** To ensure the model generalizes well to new data and maintains a similar RMSE to the training phase.\n",
    "* **The Pipeline:** I will reuse the `prepare_features` and `optimize_memory_vectorized` functions to maintain consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81f54c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG: Starting validation pipeline for 11/2025...\n",
      "LOG: Data already exists at yellow_tripdata_2025-11.parquet. Loading locally...\n",
      "LOG: Memory Optimization Report:\n",
      "   - BEFORE: 1118.86 MB\n",
      "   - AFTER: 1088.52 MB\n",
      "   - Improvement Percentage: 2.71%\n",
      "LOG: Validation complete for yellow taxi.\n",
      "LOG: Validation RMSE: 9.48 minutes\n",
      "LOG: Training RMSE was: 9.55 minutes\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Validation Configuration for November 2025\n",
    "# Use centralized config logic for out-of-sample data validation\n",
    "val_cfg = get_config(year=2025, month=11)\n",
    "\n",
    "# 2. Ingest and Prepare Validation Data\n",
    "print(f\"LOG: Starting validation pipeline for {val_cfg['month']}/{val_cfg['year']}...\")\n",
    "df_val_raw = ingest_data(val_cfg)\n",
    "\n",
    "# 3. Preprocessing and Memory Optimization\n",
    "# Ensure training and validation datasets undergo identical transformations\n",
    "df_val = preprocess_data(df_val_raw, val_cfg)\n",
    "df_val = optimize_memory_vectorized(df_val, val_cfg)\n",
    "\n",
    "# 4. Feature Transformation (CRITICAL: transform only)\n",
    "# Reuse the DictVectorizer fitted on training data to prevent feature mismatch\n",
    "# Use cast to ensure type safety without runtime overhead\n",
    "val_dicts = df_val[val_cfg[\"all_features\"]].to_dict(orient=\"records\")\n",
    "X_val = dv.transform(iter(cast(List[Dict[str, Any]], val_dicts)))\n",
    "y_val = df_val[val_cfg[\"target\"]].to_numpy()\n",
    "\n",
    "# 5. Model Evaluation\n",
    "# Generate predictions using the Linear Regression model trained on October data\n",
    "y_pred_val = lr.predict(X_val)\n",
    "rmse_val = root_mean_squared_error(y_val, y_pred_val)\n",
    "\n",
    "print(f\"LOG: Validation complete for {val_cfg['taxi_type']} taxi.\")\n",
    "print(f\"LOG: Validation RMSE: {rmse_val:.2f} minutes\")\n",
    "print(f\"LOG: Training RMSE was: {rmse_train:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-taxi-duration-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
